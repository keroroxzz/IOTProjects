{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e0c9a0a",
   "metadata": {},
   "source": [
    "# IOT-IPS\n",
    "\n",
    "The script for iot-ips project\n",
    "\n",
    "Current use a simple fully-connected classifer with averaged rssi + csi information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81e9c001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rtu/anaconda3/envs/habitat/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27635d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "myseed = 6666  # set a random seed for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(myseed)\n",
    "torch.manual_seed(myseed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d876d3ce",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c5376e",
   "metadata": {},
   "source": [
    "## Define the input format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01975ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(rssi, csi, position):\n",
    "    '''\n",
    "    description:\n",
    "        this is a function to rearrange the raw data into the form of your model input\n",
    "        so that you can easily design the input format and arrange the input data.\n",
    "        \n",
    "    input:\n",
    "        rssi: the averaged rssi in one collect file, stored in numpy array\n",
    "        csi: an array consists of all csi data in one collect file, stored in a 2D numpy array\n",
    "        position: nparray(x,y)\n",
    "        Note: both the rssi and csi belong to the same location\n",
    "        \n",
    "    output:\n",
    "        x: the list containing inputs of the model\n",
    "        y: the correspond ground truth\n",
    "    '''\n",
    "    \n",
    "    signal = np.concatenate((rssi,csi.mean(axis=0)))\n",
    "    y = np.array(position, dtype=np.int32)\n",
    "    #onehot = np.zeros((27,), dtype=int)\n",
    "    #onehot[int(position[0]+position[1]*3)]=1\n",
    "    \n",
    "    x = [signal,]\n",
    "    y = [y,]\n",
    "    \n",
    "    x = [torch.Tensor(i) for i in x]\n",
    "    y = [torch.LongTensor(i) for i in y]\n",
    "    #print(y)\n",
    "        \n",
    "    return x, y # use rssi data only as example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ff5a46",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce83f66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path, parser):\n",
    "        filenames = os.listdir(path)\n",
    "        self.parser = parser\n",
    "        \n",
    "        x_ = []\n",
    "        y_ = []\n",
    "        \n",
    "        for name in filenames:\n",
    "            if 'csi' in name:\n",
    "                file = os.path.join(path, name)\n",
    "                x,y = self.readOne(file)\n",
    "                x_.extend(x)\n",
    "                y_.extend(y)\n",
    "        \n",
    "        self.x = x_\n",
    "        self.y = y_\n",
    "                      \n",
    "    def readOne(self, fname):\n",
    "        \n",
    "        if 'csi' in fname:\n",
    "            csi = fname\n",
    "            rssi = fname.replace('csi','rssi')\n",
    "        elif 'rssi' in fname:\n",
    "            rssi = fname\n",
    "            csi = fname.replace('rssi','csi')\n",
    "        \n",
    "        # parse the position from the file name\n",
    "        position = csi[csi.find('csi')+3: csi.rfind('.')].split('_')\n",
    "        position = np.asfarray([float(i) for i in position])\n",
    "        \n",
    "        rssi_f = open(rssi)\n",
    "        csi_f = open(csi)\n",
    "        rssilines = rssi_f.readlines()\n",
    "        csilines = csi_f.readlines()\n",
    "        \n",
    "        # Read the rssi and average\n",
    "        row = 0\n",
    "        rssi_data = np.zeros(8, dtype=float)\n",
    "        for line in rssilines:\n",
    "            rssi_raw = line.split(',')\n",
    "            try:\n",
    "                rssi_data += np.asfarray([int(i) for i in rssi_raw])\n",
    "                row += 1\n",
    "            except:\n",
    "                continue\n",
    "        rssi_data /= row\n",
    "            \n",
    "        # Read the csi\n",
    "        csi_data = []\n",
    "        for line in csilines:\n",
    "            csi_raw = line.split(',')[-2][1:-2]  # skip the brackets and space\n",
    "            csi_str = csi_raw.split(' ')\n",
    "            csi_float = np.asfarray([int(i) for i in csi_str])\n",
    "            csi_data.append(csi_float)\n",
    "        csi_data = np.asfarray(csi_data)\n",
    "        \n",
    "        rssi_f.close()\n",
    "        csi_f.close()\n",
    "        \n",
    "        x,y = self.parser(rssi_data, csi_data, position)\n",
    "        \n",
    "        return x,y\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f87ffe",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21e1064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim=136, hidden=32):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        layers = [\n",
    "            nn.Linear(input_dim,hidden),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden,27),\n",
    "        ]\n",
    "        \n",
    "        self.xlayers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.xlayers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aea58a",
   "metadata": {},
   "source": [
    "# Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24716bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0005\n",
    "momentum = 0.03\n",
    "n_epochs = 10000\n",
    "batch_size = 32\n",
    "valid_epoch = 1000\n",
    "save_path = './test.pt'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "414ece05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset('data/train', parser)\n",
    "test_dataset = Dataset('data/test', parser)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "model = Model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "16324677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [51/10000]:   0%|          | 50/10000 [00:00<00:40, 247.79it/s, loss=3.32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000]: Train loss: 6.1288, Train acc: 0.0000, Valid acc: 0.0000\n",
      "Saving model with loss 6.129...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1058/10000]:  11%|▋     | 1056/10000 [00:03<00:31, 282.73it/s, loss=2.53]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1001/10000]: Train loss: 2.5689, Train acc: 0.2963, Valid acc: 0.0000\n",
      "Saving model with loss 2.569...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2060/10000]:  20%|█▏    | 2048/10000 [00:07<00:27, 291.59it/s, loss=1.85]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2001/10000]: Train loss: 1.8793, Train acc: 0.7778, Valid acc: 0.0000\n",
      "Saving model with loss 1.879...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3066/10000]:  30%|█▊    | 3045/10000 [00:10<00:23, 300.77it/s, loss=1.24]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3001/10000]: Train loss: 1.3001, Train acc: 0.9630, Valid acc: 0.0000\n",
      "Saving model with loss 1.300...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4055/10000]:  40%|██   | 4032/10000 [00:14<00:22, 262.37it/s, loss=0.876]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4001/10000]: Train loss: 0.8816, Train acc: 1.0000, Valid acc: 0.0000\n",
      "Saving model with loss 0.882...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5059/10000]:  51%|██▌  | 5054/10000 [00:17<00:16, 294.57it/s, loss=0.599]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5001/10000]: Train loss: 0.6011, Train acc: 1.0000, Valid acc: 0.0000\n",
      "Saving model with loss 0.601...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6060/10000]:  60%|███  | 6043/10000 [00:21<00:12, 306.11it/s, loss=0.424]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6001/10000]: Train loss: 0.4370, Train acc: 1.0000, Valid acc: 0.0000\n",
      "Saving model with loss 0.437...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7058/10000]:  70%|███▌ | 7037/10000 [00:24<00:10, 274.62it/s, loss=0.307]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7001/10000]: Train loss: 0.3194, Train acc: 1.0000, Valid acc: 0.0000\n",
      "Saving model with loss 0.319...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8058/10000]:  81%|████ | 8052/10000 [00:28<00:06, 282.58it/s, loss=0.244]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8001/10000]: Train loss: 0.2371, Train acc: 1.0000, Valid acc: 0.0000\n",
      "Saving model with loss 0.237...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9063/10000]:  91%|█████▍| 9052/10000 [00:31<00:03, 307.94it/s, loss=0.19]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9001/10000]: Train loss: 0.1886, Train acc: 1.0000, Valid acc: 0.0000\n",
      "Saving model with loss 0.189...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10000/10000]: 100%|███| 10000/10000 [00:34<00:00, 285.93it/s, loss=0.146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10000/10000]: Train loss: 0.1463, Train acc: 1.0000, Valid acc: 0.0000\n",
      "Saving model with loss 0.146...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum) \n",
    "\n",
    "best_loss, step, early_stop_count = math.inf, 0, 0\n",
    "\n",
    "train_pbar = tqdm(range(n_epochs), position=0, leave=True)\n",
    "for epoch in train_pbar:\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    acc_record = []\n",
    "    loss_record = []\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x+torch.rand(x.size()).to(device))  \n",
    "        \n",
    "        y = (y[:,0]+y[:,1]*3)\n",
    "        loss = criterion(pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        _,pred = torch.max(pred, 1)\n",
    "        acc = (pred==y).sum().item()\n",
    "        acc_record.append(acc)\n",
    "        loss_record.append(loss.detach().item())\n",
    "\n",
    "        train_pbar.set_postfix({'loss': loss.detach().item()})\n",
    "    train_pbar.set_description(f'Epoch [{epoch+1}/{n_epochs}]')\n",
    "    \n",
    "    mean_train_loss = sum(loss_record)/len(loss_record)\n",
    "    mean_train_acc = sum(acc_record)/len(train_dataset)\n",
    "\n",
    "    if epoch % valid_epoch==0 or epoch==n_epochs-1:\n",
    "\n",
    "        acc_record = []\n",
    "        \n",
    "        model.eval() # Set your model to evaluation mode.\n",
    "        loss_record = []\n",
    "        with torch.no_grad():\n",
    "            for x, y in valid_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                pred = model(x)\n",
    "                \n",
    "                y = (y[:,0]+y[:,1]*3)\n",
    "                _,pred = torch.max(pred, 1)\n",
    "                acc = (pred==y).sum().item()\n",
    "                acc_record.append(acc)\n",
    "\n",
    "        mean_valid_acc = sum(acc_record)/len(test_dataset)\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Train acc: {mean_train_acc:.4f}, Valid acc: {mean_valid_acc:.4f}')\n",
    "    \n",
    "        if mean_train_loss < best_loss:\n",
    "            best_loss = mean_train_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print('Saving model with loss {:.3f}...'.format(best_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a855099b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [0 6]\n",
      "GT        : [1 7]\n",
      "\n",
      "\n",
      "Prediction: [0 6]\n",
      "GT        : [-1  6]\n",
      "\n",
      "\n",
      "Prediction: [1 6]\n",
      "GT        : [4 3]\n",
      "\n",
      "\n",
      "Prediction: [0 6]\n",
      "GT        : [1 9]\n",
      "\n",
      "\n",
      "Prediction: [0 6]\n",
      "GT        : [1 5]\n",
      "\n",
      "\n",
      "Prediction: [1 4]\n",
      "GT        : [-1  9]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, pin_memory=True)\n",
    "\n",
    "model.eval() # Set your model to evaluation mode.\n",
    "loss_record = []\n",
    "val_acc = 0\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        pred = model(x)\n",
    "        _, pred = torch.max(pred, 1) \n",
    "        pred = pred.cpu().detach().numpy()[0]\n",
    "        \n",
    "        print(f'Prediction: {np.asarray([pred%3, pred//3])}')\n",
    "        print(f'GT        : {y.cpu().detach().numpy()[0]}\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "habitat",
   "language": "python",
   "name": "habitat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
